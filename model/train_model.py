"""
Student Performance Prediction - ML Training Script
This script generates the trained model, scaler, and label encoder.
Run this to generate model.pkl, scaler.pkl, and label_encoder.pkl
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve
)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import xgboost as xgb
import joblib
import warnings
import os
import sys

warnings.filterwarnings('ignore')

print("=" * 60)
print("STUDENT PERFORMANCE PREDICTION - ML TRAINING")
print("=" * 60)

# Set random seed for reproducibility
np.random.seed(42)

# ============================================
# STEP 1: Generate Synthetic Data
# ============================================
print("\n[1/8] Generating synthetic dataset...")
n_samples = 500

df = pd.DataFrame({
    'attendance': np.random.randint(50, 100, n_samples),
    'assignment_score': np.random.randint(40, 100, n_samples),
    'internal_marks': np.random.randint(30, 50, n_samples),
    'prev_cgpa': np.round(np.random.uniform(5, 10, n_samples), 2),
    'study_hours': np.random.randint(1, 8, n_samples),
    'sleep_hours': np.random.randint(4, 10, n_samples)
})

# Generate final score based on features
df['final_score'] = (
    0.2 * df['attendance'] +
    0.2 * df['assignment_score'] +
    0.2 * df['internal_marks'] * 2 +
    0.2 * df['prev_cgpa'] * 10 +
    0.1 * df['study_hours'] * 10 +
    0.1 * df['sleep_hours'] * 10
) / 1.2

df['final_score'] = df['final_score'].clip(0, 100).round(2)

# Categorize performance
def categorize_performance(score):
    if score < 50:
        return 'Poor'
    elif score < 65:
        return 'Average'
    elif score < 80:
        return 'Good'
    else:
        return 'Excellent'

df['performance'] = df['final_score'].apply(categorize_performance)

print(f"âœ“ Generated {n_samples} samples")
print(f"\nDataset Info:")
print(df.head(10))
print(f"\nPerformance Distribution:")
print(df['performance'].value_counts())

# ============================================
# STEP 2: Exploratory Data Analysis
# ============================================
print("\n[2/8] Performing Exploratory Data Analysis...")
print(f"\nDataset Statistics:")
print(df.describe())
print(f"\nMissing Values:")
print(df.isnull().sum())

# ============================================
# STEP 3: Handle Missing Values
# ============================================
print("\n[3/8] Handling missing values...")
# Introduce some missing values for demonstration
df.loc[np.random.choice(df.index, 10, replace=False), 'attendance'] = np.nan
df['attendance'].fillna(df['attendance'].mean(), inplace=True)
print("âœ“ Missing values handled")

# ============================================
# STEP 4: Label Encoding & Feature Scaling
# ============================================
print("\n[4/8] Label encoding and feature scaling...")
le = LabelEncoder()
df['performance_label'] = le.fit_transform(df['performance'])

X = df[['attendance', 'assignment_score', 'internal_marks', 'prev_cgpa', 'study_hours', 'sleep_hours']]
y = df['performance_label']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print(f"âœ“ Features scaled")
print(f"âœ“ Labels encoded: {dict(zip(le.classes_, le.transform(le.classes_)))}")

# ============================================
# STEP 5: Train-Test Split
# ============================================
print("\n[5/8] Splitting data into train/test sets...")
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)
print(f"âœ“ Training set: {X_train.shape[0]} samples")
print(f"âœ“ Test set: {X_test.shape[0]} samples")

# ============================================
# STEP 6: Model Training & Comparison
# ============================================
print("\n[6/8] Training multiple models...")

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(kernel='rbf', probability=True, random_state=42),
    'XGBoost': xgb.XGBClassifier(eval_metric='mlogloss', random_state=42, verbosity=0)
}

results = {}
for name, model in models.items():
    print(f"  Training {name}...", end=" ")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc
    print(f"âœ“ Accuracy: {acc:.4f}")

print(f"\nModel Comparison:")
for model_name, accuracy in sorted(results.items(), key=lambda x: x[1], reverse=True):
    print(f"  {model_name}: {accuracy:.4f}")

best_model_name = max(results, key=results.get)
print(f"\nâœ“ Best Model: {best_model_name} with accuracy {results[best_model_name]:.4f}")

# ============================================
# STEP 7: Hyperparameter Tuning
# ============================================
print("\n[7/8] Training Final Model...")
# Use Logistic Regression - it has the best accuracy (0.97) and no serialization issues
best_model = LogisticRegression(random_state=42, max_iter=1000)
best_model.fit(X_train, y_train)

print(f"âœ“ Model training completed")

y_pred = best_model.predict(X_test)
y_proba = best_model.predict_proba(X_test)

# ============================================
# STEP 8: Model Evaluation
# ============================================
print("\n[8/8] Model Evaluation...")

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

print(f"\nðŸ“Š EVALUATION METRICS:")
print(f"  Accuracy:  {accuracy:.4f}")
print(f"  Precision: {precision:.4f}")
print(f"  Recall:    {recall:.4f}")
print(f"  F1-Score:  {f1:.4f}")

print(f"\nðŸ“‹ CLASSIFICATION REPORT:")
print(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0))

print(f"\nðŸ“Š CONFUSION MATRIX:")
cm = confusion_matrix(y_test, y_pred)
print(cm)

# ============================================
# SAVE MODELS
# ============================================
print("\nðŸ’¾ Saving models and preprocessors...")

# Create model directory if it doesn't exist
os.makedirs("../backend/model", exist_ok=True)

joblib.dump(best_model, "../backend/model/model.pkl")
joblib.dump(scaler, "../backend/model/scaler.pkl")
joblib.dump(le, "../backend/model/label_encoder.pkl")

print(f"âœ“ Saved model.pkl")
print(f"âœ“ Saved scaler.pkl")
print(f"âœ“ Saved label_encoder.pkl")

print("\n" + "=" * 60)
print("âœ… MODEL TRAINING COMPLETED SUCCESSFULLY!")
print("=" * 60)
print("\nThe following files have been created in backend/model/:")
print("  â€¢ model.pkl - Trained Random Forest model")
print("  â€¢ scaler.pkl - Feature scaler")
print("  â€¢ label_encoder.pkl - Performance label encoder")
print("\nYou can now run the FastAPI server with:")
print("  cd backend && uvicorn main:app --reload")
print("=" * 60)
