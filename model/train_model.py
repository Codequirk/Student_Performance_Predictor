"""
Student Performance Prediction - ML Training Script
This script generates the trained model, scaler, and label encoder.
Run this to generate model.pkl, scaler.pkl, and label_encoder.pkl
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve
)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import xgboost as xgb
import joblib
import json
import warnings
import os
import sys

warnings.filterwarnings('ignore')

print("=" * 60)
print("STUDENT PERFORMANCE PREDICTION - ML TRAINING")
print("=" * 60)

# Set random seed for reproducibility
np.random.seed(42)

# ============================================
# STEP 1: Generate Synthetic Data (3000 samples)
# ============================================
print("\n[1/9] Generating synthetic dataset with 3000 samples...")
n_samples = 3000

# Generate more correlated and realistic data for better accuracy
df = pd.DataFrame({
    'attendance': np.random.randint(50, 100, n_samples),
    'assignment_score': np.random.randint(40, 100, n_samples),
    'internal_marks': np.random.randint(30, 50, n_samples),
    'prev_cgpa': np.round(np.random.uniform(5, 10, n_samples), 2),
    'study_hours': np.random.randint(1, 8, n_samples),
    'sleep_hours': np.random.randint(4, 10, n_samples)
})

# Generate final score with better correlation to features
df['final_score'] = (
    0.25 * df['attendance'] +
    0.25 * df['assignment_score'] +
    0.15 * df['internal_marks'] * 2 +
    0.15 * df['prev_cgpa'] * 10 +
    0.1 * df['study_hours'] * 10 +
    0.1 * df['sleep_hours'] * 5
) / 1.0

df['final_score'] = df['final_score'].clip(0, 100).round(2)

# Categorize performance with adjusted thresholds for better distribution
def categorize_performance(score):
    if score < 50:
        return 'Poor'
    elif score < 65:
        return 'Average'
    elif score < 80:
        return 'Good'
    else:
        return 'Excellent'

df['performance'] = df['final_score'].apply(categorize_performance)

print(f"âœ“ Generated {n_samples} samples")
print(f"\nDataset Info:")
print(df.head(10))
print(f"\nPerformance Distribution:")
print(df['performance'].value_counts())
print(f"\nDataset Shape: {df.shape}")

# ============================================
# STEP 2: Save Dataset to Database Folder
# ============================================
print("\n[2/9] Saving dataset to database folder...")
os.makedirs("../database", exist_ok=True)

# Save as JSON file
training_data = df.to_dict('records')
with open('../database/training_data_3000.json', 'w') as f:
    json.dump(training_data, f, indent=2)

print(f"âœ“ Saved training_data_3000.json to database folder")
print(f"  File contains {len(training_data)} student records")

# ============================================
# STEP 3: Exploratory Data Analysis
# ============================================
print("\n[3/9] Performing Exploratory Data Analysis...")
print(f"\nDataset Statistics:")
print(df.describe())
print(f"\nMissing Values:")
print(df.isnull().sum())

# ============================================
# STEP 4: Handle Missing Values
# ============================================
print("\n[4/9] Handling missing values...")
# Introduce some missing values for demonstration
df.loc[np.random.choice(df.index, 10, replace=False), 'attendance'] = np.nan
df['attendance'].fillna(df['attendance'].mean(), inplace=True)
print("âœ“ Missing values handled")

# ============================================
# STEP 5: Label Encoding & Feature Scaling
# ============================================
print("\n[5/9] Label encoding and feature scaling...")
le = LabelEncoder()
df['performance_label'] = le.fit_transform(df['performance'])

X = df[['attendance', 'assignment_score', 'internal_marks', 'prev_cgpa', 'study_hours', 'sleep_hours']]
y = df['performance_label']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print(f"âœ“ Features scaled")
print(f"âœ“ Labels encoded: {dict(zip(le.classes_, le.transform(le.classes_)))}")

# ============================================
# STEP 6: Train-Test Split
# ============================================
print("\n[6/9] Splitting data into train/test sets...")
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)
print(f"âœ“ Training set: {X_train.shape[0]} samples")
print(f"âœ“ Test set: {X_test.shape[0]} samples")

# ============================================
# STEP 7: Model Training & Comparison
# ============================================
print("\n[7/9] Training multiple models...")

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(kernel='rbf', probability=True, random_state=42),
    'XGBoost': xgb.XGBClassifier(eval_metric='mlogloss', random_state=42, verbosity=0)
}

results = {}
for name, model in models.items():
    print(f"  Training {name}...", end=" ")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc
    print(f"âœ“ Accuracy: {acc:.4f}")

print(f"\nModel Comparison:")
for model_name, accuracy in sorted(results.items(), key=lambda x: x[1], reverse=True):
    print(f"  {model_name}: {accuracy:.4f}")

best_model_name = max(results, key=results.get)
print(f"\nâœ“ Best Model: {best_model_name} with accuracy {results[best_model_name]:.4f}")

# ============================================
# STEP 8: Hyperparameter Tuning
# ============================================
print("\n[8/9] Training Final Model with Hyperparameter Tuning...")
# Use Random Forest with optimized hyperparameters for better accuracy
best_model = RandomForestClassifier(
    n_estimators=200,  # Increased from 100
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)
best_model.fit(X_train, y_train)

print(f"âœ“ Model training completed")

y_pred = best_model.predict(X_test)
y_proba = best_model.predict_proba(X_test)

# ============================================
# STEP 9: Model Evaluation
# ============================================
print("\n[9/9] Model Evaluation...")

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

print(f"\nðŸ“Š EVALUATION METRICS:")
print(f"  Accuracy:  {accuracy:.4f}")
print(f"  Precision: {precision:.4f}")
print(f"  Recall:    {recall:.4f}")
print(f"  F1-Score:  {f1:.4f}")

print(f"\nðŸ“‹ CLASSIFICATION REPORT:")
print(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0))

print(f"\nðŸ“Š CONFUSION MATRIX:")
cm = confusion_matrix(y_test, y_pred)
print(cm)

# ============================================
# SAVE MODELS
# ============================================
print("\nðŸ’¾ Saving models and preprocessors...")

# Create model directory if it doesn't exist
os.makedirs("../backend/model", exist_ok=True)

joblib.dump(best_model, "../backend/model/model.pkl")
joblib.dump(scaler, "../backend/model/scaler.pkl")
joblib.dump(le, "../backend/model/label_encoder.pkl")

print(f"âœ“ Saved model.pkl")
print(f"âœ“ Saved scaler.pkl")
print(f"âœ“ Saved label_encoder.pkl")

print("\n" + "=" * 60)
print("âœ… MODEL TRAINING COMPLETED SUCCESSFULLY!")
print("=" * 60)
print(f"\nðŸ“Š TRAINING SUMMARY:")
print(f"  Total Samples Generated: 3000")
print(f"  Training Samples: {X_train.shape[0]}")
print(f"  Test Samples: {X_test.shape[0]}")
print(f"  Model: Random Forest Classifier (200 estimators)")
print(f"  Final Accuracy: {accuracy:.4f}")
print("\nThe following files have been created:")
print("  â€¢ backend/model/model.pkl - Trained Random Forest model")
print("  â€¢ backend/model/scaler.pkl - Feature scaler")
print("  â€¢ backend/model/label_encoder.pkl - Performance label encoder")
print("  â€¢ database/training_data_3000.json - 3000 training samples")
print("\nYou can now run the FastAPI server with:")
print("  cd backend && uvicorn main:app --reload")
print("=" * 60)
